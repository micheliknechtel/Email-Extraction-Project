Received: (qmail 11919 invoked from network); 9 Jan 2003 00:32:13 -0000
Received: from dev212.mailshell.com (HELO mailshell.com) (71.129.195.163)
  by dev50.mailshell.com with SMTP; 9 Jan 2003 00:32:13 -0000
Received: (qmail 31357 invoked by uid 99); 9 Jan 2003 00:32:12 -0000
Received: (qmail 14155 invoked from network); 9 Jan 2003 00:32:11 -0000
Received: from unknown (HELO Post-Office.UH.EDU) (129.7.1.20)
  by www.mailshell.com with SMTP; 9 Jan 2003 00:32:11 -0000
Received: from Post-Office.UH.EDU (localhost [127.0.0.1])
 by Post-Office.UH.EDU (PMDF V6.0-025 #47183)
 with ESMTP id <0H8E060MZ4PQLJ@Post-Office.UH.EDU>; Wed,
 08 Jan 2003 18:31:07 -0600 (CST)
Received: from LISTSERV.UH.EDU by LISTSERV.UH.EDU
 (LISTSERV-TCP/IP release 1.8d) with spool id 921842 for
 PSYCHE-L@LISTSERV.UH.EDU; Wed, 08 Jan 2003 18:30:27 -0600
Received: from goethe.klab.caltech.edu
 (goethe.klab.caltech.edu [131.215.31.40]) by Post-Office.UH.EDU
 (PMDF V6.0-025 #47183) with ESMTP id <0H8F09RJW7ERUB@Post-Office.UH.EDU> for
 psyche-l@LISTSERV.UH.EDU; Wed, 08 Jan 2003 18:16:04 -0600 (CST)
Received: from [131.215.31.184] (DHCP-31-184.caltech.edu [131.215.31.184])
 by goethe.klab.caltech.edu (Postfix) with ESMTP id 534D3A9660 for
 <psyche-l@LISTSERV.UH.EDU>; Wed, 08 Jan 2003 16:16:03 -0800 (PST)
Date: Wed, 08 Jan 2003 16:16:43 -0800
From: Patrick Wilken <patrickw@KLAB.CALTECH.EDU>
Subject: Psyche 9(01): Neither *Hot* nor *Cold*: An Alternative Account of
 Consciousness by Robert W. Lurz
Sender: "PSYCHE: an interdisciplinary journal of research on              consciousness" <PSYCHE-L@LISTSERV.UH.EDU>
X-Sender: patrickw@goethe.klab.caltech.edu
Approved-by: patrickw@KLAB.CALTECH.EDU
To: "Mailshell User" <nospam@mailshell.com>
Reply-to: Psyche-B <PSYCHE-B@LISTSERV.UH.EDU>
Message-id: <a05111b00ba426f94398f@[131.215.31.184]>
MIME-version: 1.0
Content-type: text/plain; charset=us-ascii; format=flowed

-----------------------------------------------------------------
PSYCHE: AN INTERDISCIPLINARY JOURNAL OF RESEARCH ON CONSCIOUSNESS
-----------------------------------------------------------------

NEITHER *HOT* NOR *COLD*: AN ALTERNATIVE ACCOUNT OF CONSCIOUSNESS

   Robert W. Lurz
   Department of Philosophy
   Brooklyn College, CUNY
   2900 Bedford Avenue
   Brooklyn, NY 11210
   U.S.A.

   rlurz@brooklyn.cuny.edu

   Copyright (c) Robert W. Lurz 2003

   PSYCHE, 9(01), January 2003
   http://psyche.cs.monash.edu.au/v9/psyche-9-01-lurz.html

   KEYWORDS: animal consciousness, conscious and unconscious mental
   states, higher-order thoughts.

   ABSTRACT: I identify three dominant positions in the philosophy of
   mind on the nature and distribution of consciousness: the exclusive
   HOT position, the inclusive HOT position, and the COLD position. I
   argue that each of these positions has its own rather counterintuitive
   consequence and, as a result, is not entirely satisfying. To avoid
   these consequences, I argue, a common assumption of the dominant
   positions ought to be rejected -- namely, that to be conscious *of*
   one's mental states is to be conscious *that* one has them. I go on to
   show that once this assumption is rejected, an alternative account of
   consciousness --  the SO account -- emerges. I develop the SO account
   in the latter half of the paper, showing how it offers a plausible
   explanation of the difference between conscious and unconscious mental
   states.


1. INTRODUCTION

There are three dominant positions in the philosophy of mind on the
nature and distribution of consciousness that can be identified by their
responses to the following inconsistent quartet of propositions:

     1. Conscious mental states are simply mental states of which one is
     conscious.

     2. To be conscious of one's mental states is to be conscious that
     one has them.

     3. Animals have conscious mental states.

     4. Animals are not conscious that they have mental states.

Proponents of the first position take propositions 1, 2 and 4 to be true
and conclude that proposition 3 is false: the mental states of animals,
insofar as they have them, are unconscious. Proponents of the second
position take propositions 1, 2 and 3 to be true and concluded that
proposition 4 is false: animals are conscious that they have mental
states. And proponents of the third position take propositions 2, 3, and
4 to be true and conclude that proposition 1 is false: having conscious
mental states does not require being conscious of them.

Proponents of the first two positions typically endorse some version of
the higher- order thought (HOT) theory of consciousness; so I shall call
these first two positions *HOT positions*. To distinguish between them,
I shall call the former the *inclusive* HOT position, since it includes
animals in the class of creatures capable of being conscious that they
have mental states, and the latter the *exclusive* HOT position, since
it excludes animals from this class. Peter Carruthers (2000) is perhaps
the best-known proponent of the exclusive HOT position, while David
Rosenthal (1986, 1997) is perhaps the best-known proponent of the
inclusive HOT position.

The third position, unfortunately, lacks a consistent title in the
literature. For the sake of this paper, however, I shall call it the
*COLD position*, since its proponents, in rejecting proposition 1,
maintain that being *c*onscious *o*f one's mental states *l*acks the
power, as it were, to *d*ifferentiate conscious from unconscious states.
  Fred Dretske (1995) is perhaps the best-known proponent of the COLD
position.

I argue that neither of these positions is entirely satisfying, since
the propositions they are forced to conclude are false -- namely,
proposition 1, 3, and 4 -- are all intuitively plausible; and that all
three positions mistakenly assume, in their endorsement of proposition
2, that the only way to be conscious of one's mental states is to be
conscious that one has them. Once this mistake is exposed, I argue, an
alternative way of thinking about consciousness emerges that is
perfectly consistent with the truth of propositions 1, 3, and 4.  My
intention here is not to conclusively refute each of the three positions
above -- I'm not sure that such a thing could be done. Rather, my
intention is to introduce and motivate an alternative view of
consciousness and, thereby, to advance the current debate in the
philosophy of mind over the nature of mental state consciousness.


2. STATE CONSCIOUSNESS

Some distinctions are in order before we continue. First, it is
generally recognized that there are two different kinds of things,
creatures and mental states, that can be conscious and, thereby, two
different kinds of consciousness: *creature consciousness* and *mental
state consciousness* ('state consciousness' for short). Creature
consciousness comes in two varieties. A creature can be conscious in the
sense of being awake, as opposed to being asleep or comatose, and it can
be conscious in the sense of being conscious *of* something. Since the
former variety of creature consciousness does not require the deployment
of a direct object after 'conscious,' while the latter variety does, the
former is typically called *intransitive* creature consciousness, while
the latter is typically called *transitive* creature conscious.<1>

Now, there are also two different varieties of transitive creature
consciousness, since there are two different kinds of things of which a
creature can be transitively conscious: things outside its mind and
things inside its mind. A creature that is conscious of something
outside its mind -- say, some event or object in its environment -- is
said to be *outwardly* conscious of it; whereas, a creature that is
conscious of something inside its mind -- say, a thought or an
experience -- is said to be *inwardly* conscious of it.<2>

Mental states, as noted above, are also said to be conscious. We say,
for instance, that John's auditory experience of the ticking of the
clock is conscious (as opposed to saying that it is unconscious or
subliminal), or that Mary has the conscious thought that she married the
wrong person (as opposed to saying that she has this thought
unconsciously -- say, as the result of some act of self-deception or
denial). State consciousness admits of only an intransitive variety,
however. We say that John's auditory experience is conscious, *full
stop*; or that Mary's thought is conscious, *full stop*. We do not say
that John's auditory experience is conscious *of* something, or that
Mary's thought is conscious *of* something. The ordinary use of
'conscious' does not take a direct object when applied to mental states,
and in that sense, it is used intransitively.<3>

It is with the nature of state consciousness that the three positions
mentioned above are concerned. And all three endorse some explanation of
the nature of state consciousness in terms of one variety or the other
of transitive creature consciousness. The HOT positions, for instance,
endorse the view that what makes a mental state conscious is the fact
that its possessor is inwardly conscious of it. Rosenthal (1997), for
instance, writes that "a mental state is intransitively conscious just
in case we are transitively conscious of it" (p. 737). HOT theorists, of
course, go on to identify our being transitively conscious of our mental
states with our being conscious that we have them -- that is, with our
having non-inferentially based higher-order thoughts to the effect that
we have such states. Again, Rosenthal (1987) writes that since "in
general, our being conscious of something is just a matter of our having
a thought of some sort about it[,]... it is natural to identify a mental
state's being conscious with one's having a roughly contemporaneous
thought that one is in that mental state" (p. 335).<4>

The COLD position, on the other hand, endorses the view that what makes
a mental state conscious is the fact that the mental state itself makes
its possessor outwardly conscious of some item or fact in the
environment. Dretske (1997), for instance, writes that "[w]hat makes
[mental states] conscious is not S's awareness of them, but their role
in making S conscious -- typically (in the case of sense perception), of
some external object" (p. 6).

Although the COLD theorist disagrees with the HOT theorists over what
kind of transitive creature consciousness is necessary and sufficient
for state consciousness, he agrees with the HOT theorists that to be
inwardly conscious of one's mental states is to be conscious that one
has them. "The mind's awareness of itself," Dretske (1999) maintains,
"is an awareness of facts about itself, an awareness that internal
experience, *e* is *B*" (p. 2). The COLD theorist and the HOT theorists,
then, are in apparent agreement over one thing: that proposition 2 above
is true.

I shall argue that these positions are mistaken on this point of
agreement, and that, as a result, they all share a common problem.
However, before I argue for this, I wish to show how each position has
its own unique counterintuitive consequences.


3. HOT CONSEQUENCES

Recall that the exclusive HOT position is forced to conclude that the
mental states of animals, in so far as they have them, are unconscious.
But, this consequence is rather counterintuitive on the face of it. It
seems plausible that some animals have conscious perceptual experiences
and beliefs about items in their environments. My reason for claiming
this is not that I have succeeded in imagining what it is like for
animals to perceive or have beliefs about items in their environments --
for I have not. Rather, my reason for claiming this is simply that a lot
of animal behavior can be predicted and explained quite well in terms of
the concepts and generalizations of our folk psychology, and that these
concepts and generalizations are about *conscious* mental states.

For instance, when my wife asks me why Fred (our neighbor) and Fido (his
dog) are looking into the branches of our oak tree, and I reply,
"Because they saw Minx (our cat) run up the tree and think that she's
still up there," I am using the same folk- psychological concepts (e.g.,
'see' and 'think') and the same (implicit) folk- psychological
generalizations (e.g., one will think that *x* is in location *y* if one
just saw *x* go into location *y*, *ceteris paribus*) to explain Fred's
and Fido's respective behaviors; and these concepts and generalizations
are about conscious mental states. I have no idea what Fred or Fido
would do if they had unconsciously saw Minx run up the tree and had
unconsciously thought she was still up there.  Our folk theory is a
theory of how conscious mental states interact and produce behavior. It
is not a theory about how unconscious mental states interact and produce
behavior. Unconscious mental states fall under their own laws, and it is
the project of scientific psychology to discover them. Therefore, since
our folk theory does a fairly good job in explaining and predicting
quite a lot of animal behavior, we have some reason to believe that
animals have the kinds of mental states that our folk theory's concepts
and generalizations are about -- namely, conscious mental states.

Peter Carruthers (2000) disagrees.  He argues that there is no
explanatory need to attribute conscious mental states to animals:

     Everything that the cat does can be explained perfectly well by
     attributing beliefs, desires, and perceptions to it. There is no
     explanatory necessity to attribute *conscious* beliefs, desires, or
     perceptions. All we really have reason to suppose, in fact, is that
     the cat *perceives* the smell of the cheese. We have no independent
     grounds for thinking that its percepts will be phenomenally
     conscious ones. Certainly such grounds are not provided by the need
     to explain the cat's behaviour. For this purpose the concept of
     perception, *simpliciter*, will do perfectly well. (p. 199)

I agree with Carruthers that it's not necessary for me to say to my wife
that Fred and Fido consciously saw Minx run up the tree and consciously
think that she is still up there. I could, and did, simply say that they
saw Minx run up the tree and think that she is still up there.  But,
this in no way indicates that I was explaining Fred's and Fido's
behaviors in terms of something other than the mental states that are
picked out by the concepts and generalizations of our ordinary folk
psychology. I was not explaining their behaviors in terms of mental
states that are neither conscious nor unconscious -- for there are no
such mental states -- or in terms of unconscious mental states -- for I
do not know the laws that describe the functions of such mental states.

Carruthers' point, however, might be that we could explain and predict
animal behavior just as well with a theory whose concepts and
generalizations are about unconscious mental states and processes.  But
clearly the burden is on Carruthers to show that there is such a theory
which explains and predicts animal behavior just as well as our folk
theory.  As far as I know, there is no such theory in the offing. So, as
things stand presently, we seem to have more reason to believe that
animals have conscious mental states than that they do not.

The inclusive HOT position, on the other hand, endorses the view that
animals have conscious mental states. But in doing so, it commits itself
to the view that animals are conscious that they have mental states. And
such a view, I believe, is rather implausible. For to be conscious that
one has mental states is to know and, therefore, to think that one has
mental states; and to think that one has mental states involves having
higher-order thoughts about one's lower-order mental states. But such
higher-order thoughts seem much too sophisticate for many (if any)
animals to possess, since they would be about states internal and
unobservable to the animal itself. Recall that in its endorsement of
proposition 2 above, the inclusive HOT position (as well as the
exclusive HOT position) is committed to the view that the only type of
awareness one has of one's own mental states is a cognitive awareness
*that* one has them. There is, in other words, no observational
awareness of one's mental states that occurs between one's mental states
and one's cognitive awareness that one has them. Consequently, on the
inclusive HOT position, to be conscious that one has mental states is to
have thoughts about internal states that are unobservable to oneself,
much like having thoughts about states of one's own brain or heart.

But, it is certainly implausible to suppose that animals have thoughts
-- even very crude ones -- about unobservable states occurring inside
their own bodies. It is intuitively implausible to suppose, for
instance, that my cat has thoughts -- even very crude ones -- about what
is going on inside her own brain or heart. How would she come to have
such thoughts? She would not acquire them through observation, since my
cat does not observer what is going on inside her own brain or heart.
And it is intuitively implausible to suppose that she acquires them
through some sort of hypothesis-to-the-best-explanation process. For it
is implausible to suppose that my cat engages in such explanatory
processes at all.<5>  But by parity of reason, then, it seems just as
implausible to suppose that my cat -- or any animal for that matter --
has thoughts about unobservable states occurring inside her own mind.

Rosenthal (1986) disagrees, at least with respect to the plausibility of
animals having thoughts about their own sensory states. He writes:

     One need not have much ability to think to be able to have a thought
     that one is in a particular sensation. Infants and nonhuman animals
     can discriminate among external objects, and master regularities
     pertaining to them. So most of these beings can presumably form
     thoughts about such objects, albeit primitive thoughts that are very
     likely not conscious. No more is needed to have thoughts about one's
     more salient sensory experiences. (p. 350)

I do not find Rosenthal's analogy here particularly persuasive, however.
I agree that it is likely that animals discriminate among external
objects and master regularities pertaining to them if they *observe*
these objects. But if they cannot observe these objects (because, say,
they are inside their own or other creature's bodies), then it is not at
all clear that they would be able to discriminate among them and master
regularities pertaining to them. How would they perform such
discriminations? Rosenthal's analogy has force only on the assumption
that animals *observe* the sensory experiences about which they
(putatively) discriminate and master regularities. But, of course, an
inclusive HOT theorist, such as Rosenthal, cannot make such an
assumption. One's mental states, according to the inclusive HOT
position, are only thought about, not observed by, oneself.


4. COLD CONSEQUENCE

It seems, then, that both HOT positions have rather counterintuitive
consequences. It might be thought, then, that the COLD position is the
position to endorse. After all, the COLD position accepts the claim that
animals have conscious mental states and denies the claim that they are
conscious that they have such states. The COLD position, however, has
its own unique counterintuitive consequence: the denial of proposition 1
above.

My only proof that proposition 1 is intuitively plausible -- and, hence,
that its denial is counterintuitive -- is that it seems to me, and to
others (Rosenthal, 1997, p. 738; Lycan, 1996, p. 25), that part of our
folk-psychological concept of state consciousness is that conscious
mental states are mental states of which a subject is conscious. It
simply sounds strange to my ears to say that a creature could have a
conscious thought or experience of which it was not at all conscious.

Of course, appearances are sometimes deceiving. So, I admit that I could
be mistaken in my intuition about what we mean when we say that a
creature's mental state is conscious. And in fact, Fred Dretske, in
defending the COLD position against the sort of objection I raise here,
makes a few remarks that look to be aimed at challenging the truth of
proposition 1. Yet when examined closely, Dretske's remarks, I believe,
amount to something less then a compelling argument.

Here are the remarks that Dretske (1995) makes which appears to be
directed at undermining the truth of proposition 1:

         Some people have cancer and they are conscious of having it.
         Others have it, but are not conscious of having it. Are there,
         then, two forms of cancer: conscious and unconscious cancers?

         Some people are conscious of having experiences. Others have
         them, but are not conscious of having them. Are there, then, two
         sorts of experiences: conscious and unconscious experiences?

         Experiences are, in this respect, like cancers. Some of them we
         are conscious of having, others we are not. But the difference
         is not a difference in the experience. It is a difference in the
         experiencer -- a difference in what the person knows about the
         experience he or she is having. (p. 97)

On one interpretation -- at least, one which is relevant to our purposes
-- Dretske appears to be suggesting something like the following
argument: if conscious experiences are simply experiences of which one
is conscious, then there would be experiences of different sorts --
conscious and unconscious ones -- without there being any difference
*in* those experiences<6> ; but, that would be just as absurd as there
being cancers of different sorts without there being any difference *in*
those cancers; and, therefore, it is not the case, as proposition 1
entails, that conscious experiences are experiences of which one is
conscious.

Dretske's argument, assuming that I have interpreted it correctly,
appears to rest upon a rather dubious assumption -- namely, that it is
impossible for there to be different sorts of things (e.g., experiences,
cancers, and so forth) without their being a difference *in* those
things. This, however, does not appear to be impossible. There may, for
instance, be no difference *in* the rocks on earth and those on the
moon. They may be composed of precisely the same elements and minerals,
say. And yet the rocks on the moon are lighter than those on the earth.
Hence, there are different sorts of rocks -- heavy rocks and light rocks
-- without there being a difference *in* those rocks. Or consider the
fact that there may be no difference *in* the paintings of Rembrandt and
those of a Rembrandt forger. The paintings may share all the same
arrangements of color, shapes, brushstrokes, and so on. And yet the
former paintings are Rembrandts, while the latter are not. And so there
are different sorts of paintings -- Rembrandts and non-Rembrandts --
without there being a difference *in* those paintings.

Why, then, can't the same be said of experiences? There are different
sorts of experiences -- conscious and unconscious ones -- without there
being a difference *in* those experiences. This would, of course,
suggest that state consciousness is an extrinsic (or relational)
property of experiences, as heaviness and lightness are extrinsic
properties of rocks, and Rembrandtness and non-Rembrandtness are
extrinsic properties of paintings. But, what of it? In fact, this seems
to be the very idea behind the truth of proposition 1: being conscious
of one's thoughts and experiences is an extrinsic property of one's
thoughts and experiences, and saying that one's thoughts and experiences
are conscious is simply saying that they have this extrinsic property.
Dretske's argument, then, appears to assume that state consciousness is
not an extrinsic property, and such an assumption, it would appear, begs
the question against the very idea behind the truth of proposition 1.


5. NEITHER *HOT* NOR *COLD*

I'm not convinced, then, that Dretske's argument provides a good reason
to doubt my intuition in the truth of proposition 1. But now where does
this leave us? If propositions 1, 3, and 4 above are all intuitively
plausible, should we, then, conclude that proposition 2 -- the
proposition that to be conscious of one's mental states is to be
conscious that one has them -- is false? I believe so. But, if
proposition 2 is rejected, it might be wondered: what is it, then, to be
conscious of one's mental states? And, it might be further thought that
unless an answer can be given to this question, perhaps it is wiser to
accept proposition 2 and simply bite the bullet and deny one of the
remaining propositions 1, 3, or 4 -- despite their intuitive appeal.

I don't disagree with this line of reasoning, but I also don't think
that we have to bite any bullets. For I believe that there is another
way to be conscious of one's mental states which does not amount to
being conscious that one has them and is, therefore, perfectly
consistent with the truth of propositions 1, 3, and 4: one can simply be
conscious *of what* one's mental states represent. My claim is that a
creature can be conscious *of* its thoughts and experiences simply by
being conscious *of what* it thinks or experiences in having those
thoughts or experiences, and that its being conscious *of what* it
thinks or experiences does not entail its being conscious *that* it
thinks or has experiences. To give some intuitive support for this
claim, consider an analogous claim regarding paintings.

It seems plain that in order to see *what* a particular painting
represents, one must see the painting itself. If one does not see the
painting itself -- say, if one is looking in the wrong direction, or is
seeing a different painting, or is blind -- then one cannot be said to
see *what* that particular painting represents. But, to see *what* a
particular painting represents does not necessarily require seeing *that
a particular painting* represents something (or even *that there is a
particular painting present*). When we are fooled by a *trompe l'oeil*
painting, for instance, we do not see that a painting represents
something (or even that there is a painting present)<7> -- otherwise, we
would not be fooled. Rather, in such cases, we simply see *what* the
painting represents. And so it appears that, with respect to *trompe
l'oeil* paintings, one can see (be aware of) a particular painting by
seeing *what* (being aware *of what*) it represents without seeing
*that* (being aware *that*) a painting represents something (or even
*that* there is a painting present).

An important point of clarification should be made before we move on.
The phrase, "what the painting represents," is ambiguous. It could be
taken to refer to the actual object (person, place, or thing) the
painting is based on or to (what philosophers call) the intentional
content of the painting. Three identical-looking paintings by different
artists, for example, may each depict a woman seated before an open
window, and yet it may be that neither of the paintings is based on the
same woman: two of the artists, we may suppose, had different women
modeling for them, and one artist simply created his painting from his
imagination. So, in one sense of the phrase "what the painting
represents," the intentional-content sense, what these three paintings
represent is the same: a woman seated by an open window. But, on the
other sense of this phrase, the actual-object sense, what these
paintings represent is not the same, since different women were the
models for two of the paintings, and no woman was a model for the other.
It is the intentional-content sense of the phrase "what the painting
represents" that is being used in the *trompe l'oeil* example above. To
see what (be aware *of what*) a particular *trompe l'oeil* painting
represents is to see what (be aware *of what*) intentional content that
particular painting has.

There is a similar ambiguity with the phrase "what one thinks or
experiences." The phrase may refer to the actual, distal object of one's
thought or experience, or it may refer to the intentional content of
one's thought or experience. What three different people experience or
think, for example, may be said to be the same: each may see a red
tomato on a table, and each may think that the tomato on the table would
be good to eat; and yet the actual, distal objects of their respective
experiences and thoughts may not be the same: two of the people may be
looking at different tomatoes, and the other may be hallucinating. It is
the intentional-content sense of the phrase, "what one thinks or
experiences," that I use when speaking of a creature being conscious *of
what* it thinks or experiences. Therefore, since what the three people
above experience and think is, with respect to the intentional contents
of their respective experiences and thoughts, the same, what they are
conscious of when they are conscious *of what* they experience or think
is also the same.

Now, I believe that just as one can be aware of a particular painting by
being aware *of what* it represents without being aware *that* a
painting represents something (or even *that* there is a painting
present), a creature can be conscious of its own particular thoughts and
experiences by being conscious *of what* they represent without its
being conscious *that* it has thoughts or experiences. Consider, for
instance, a two- and-a-half-year-old child who, pointing at her dolly,
says, "that's my dolly." The child certainly appears to be conscious *of
what* she believes (namely, that that is her dolly), as is evidenced by
the fact that she says what she believes.<8>  It seems unlikely that the
child would be able to say what she believes if she were not conscious
*of what* she believes.<9>  But though it is very likely that the child
is conscious *of what* she believes, it is not at all obvious that she
is conscious *that* she *believes* it. On the contrary, it seems quite
possible that this child is not at all aware of the fact that she has
beliefs. For she very well may not have the concepts required to think
about her beliefs.<10>  Therefore, it seem quite possible that the child
is conscious *of what* she believes but is not conscious *that* she
believes it.

But in being conscious *of what* she believes, the child is conscious
*of what* a particular belief of hers represents (namely, the belief
that that is her dolly). She is, in other words, conscious *of what* a
token mental state (of a certain type) of hers represents. It is
difficult, however, to understand how she could be conscious *of what* a
token mental state of hers represents if she were not conscious of the
mental state itself. How could one see what (be aware *of what*) a
particular painting represented, or hear what (be aware *of what*) a
particular utterance meant, if one did not see (was not aware of) the
painting itself, or did not hear (was not aware of) the utterance
itself? A painting and an utterance, of course, are different from a
belief. But each is an individual representation, and as such, each
seems to require one's awareness of it in order to be aware *of what* it
(as opposed to some other token representation) represents. Therefore,
in being conscious *of what* she believes, the child is, in some sense,
conscious of a particular belief of hers -- though, she is not so by
being conscious *that* she has such a belief.<11>

Consider, as well, the case of an animal that is conscious *of what* it
perceives without its being conscious *that* it perceives. My cat, for
instance, upon espying movement in the bushes, behaves in such a way
that it seems quite appropriate to say of her that she is paying
attention to what she is seeing -- namely, the movement in the bushes.
But surely, if she were completely unaware *of what* she was seeing, she
would not be able to attend to what she was seeing.<12>  So, since it is
plausible to say that my cat is paying attention to what she is seeing,
it is plausible to say that she is (to some degree at least) conscious
*of what* she is seeing. However, it is rather implausible that my cat
is conscious *that* she *sees* movement in the bushes, since it is
rather implausible to suppose, as we saw above, that my cat has thoughts
about her own mental states. Nevertheless, in being conscious *of what*
she is seeing, my cat is conscious *of what* a token visual state of
hers represents. And, again, it is hard to understand how my cat could
be conscious *of what* a token mental state of hers represents if she
were not, in some way, conscious of the mental state itself. Therefore,
in being conscious *of what* she sees, my cat is, in some sense,
conscious of a particular visual state that she is in -- though, she is
not so by being conscious *that* she is in such a visual state.


6. AN ALTERNATIVE ACCOUNT OF CONSCIOUSNESS

If the distinctions I have drawn above are genuine, then there appear to
be two varieties of inward consciousness. A creature can be inwardly
conscious of its thoughts and experiences by being conscious *that* it
has them, or it can be inwardly conscious of its thoughts and
experiences by being conscious *of what* they represent. Since the
former variety of inward consciousness requires a creature to know and,
therefore, to think that it has thoughts and experiences, we can perhaps
appropriately label it as *higher-order* inward consciousness. The other
variety of inward consciousness, as we have seen, does not require that
a creature know or think that it has thoughts or experiences. It merely
requires that it be conscious *of what* its thoughts and experiences
represent. And since what one is conscious of, when one is conscious *of
what* one's thoughts and experiences represent, is *what* one's thoughts
and experiences represent<13> -- that is, the very same order of things
and properties that are contained in the intentional contents of one's
thoughts or experiences -- perhaps we can appropriately label this
variety of inward consciousness as *same-order* inward consciousness.

In their acceptance of proposition 2, proponents of the HOT positions
and those of the COLD position assume that there is but one type of
inward consciousness -- namely, the higher-order variety. However, we
have seen that the intuitive plausibility of propositions 1, 3, and 4
gives some motivation for rejecting this assumption and proposition 2
which rests upon it. But, if being conscious of one's thoughts and
experiences is not being higher-order conscious *that* one has them,
then what, one might wonder, is it to be conscious of one's thoughts and
experiences? The answer, I submit, is that it is to be same-order
conscious of them -- that is, it is to be conscious *of what* they
represent.

Now, if this submission is granted, then an alternative account of state
consciousness emerges, one which is neither endorsed by the HOT
positions nor the COLD position.  On this account, which we can call the
same-order (SO) account, what makes a creature's thoughts or experiences
conscious is the fact that the creature is conscious *of what* (not
*that*) its thoughts or experiences represent. On the SO account,
creatures that are conscious *of what* they think and experience have
conscious thoughts and experiences, and those that are not conscious *of
what* they think or experiences have unconscious thoughts and
experiences.

To illustrate the SO accounts' explanation of the difference between
conscious and unconscious mental states, consider a well-known case of
unconscious perception: blindsight. Due to damage to parts of their
visual cortex, blindsight subjects have blind regions (scotomas) in
their visual field. Items that fall within a blindsight subject's
scotoma are treated by the subjects as if they are not seen.
Nevertheless, on forced- choice test trials, blindsight subjects are
able to guess correctly (from a list of options) what feature an item
within their scotoma has -- though, they sincerely declare that they are
not aware of any such item. These results have led researchers (Marcel,
1998; Weiskrantz, 1986) to hypothesize that blindsight subject have
unconscious visual perceptions of certain features and items within
their scotomas.

Most of us, fortunately, do not suffer from blindsight. Items placed in
our field of vision are (under normal conditions) consciously perceived.
The SO account offers the following explanation of the relevant
difference between blindsight subjects and normally sighted subjects.
Blindsight subjects are not conscious *of what*  they are perceiving
when they are undergoing an unconscious perception of some object or
feature in their scotoma, as is evidenced by the fact that they
sincerely declare that there is no such object or feature in that region
of their visual field. It is this fact -- the fact that these subjects
are not conscious *of what* they are perceiving -- that, on the SO
account, makes the perceptual states of blindsight subjects
unconscious.<14> Normal sighted individuals, on the other hand, are
quite aware *of what* they are perceiving when they are undergoing a
conscious perceptual state, as evidenced by the fact that they are quite able
to report sincerely and spontaneously on what they are perceiving. It is
this fact -- the fact that these individuals are conscious *of what*
they are perceiving -- that, on the SO account, makes their perceptual
states conscious.

In addition to cases of unconscious perception, there are numerous
scientific cases that strongly suggest that people undergo unconscious
thoughts and thought processes. Nisbett and Wilson (1977), for example,
describe a number of studies in which people behave in particular ways
as an apparent result of certain unconscious beliefs that they possess.
In one such study, subjects were given a series of electric shocks of
increasing intensity. Half of the subjects were given a placebo pill
which they were told would produce symptoms such as heart palpitations,
breathing irregularities, hand tremors, etc. -- symptoms, that is, that
are typical of electric shocks. It was predicted that the subjects that
were given the pill would form the unconscious belief that their
shock-related symptoms were due to the pill (and not to the electric
shock) and, as a result, would tolerate more amperage than the subjects
that were not given the pill. And this is precisely what happened. The
pill subjects took four-times as much amperage as the non- pill
subjects; and yet in the interview process that immediately followed the
test trial, the pill subjects firmly denied that the pill had any such
affect on their behavior.

Of course, many of our beliefs are conscious, especially those that we
linguistically express. The SO account offers the following explanation
of the difference between our conscious beliefs and our unconscious
beliefs, such as those possessed by the subjects in Nisbett and Wilson's
experiment. The subjects in the Nisbett and Wilson's experiment
sincerely deny what they in fact seem to believe about the pill's affect
on their behavior. And their sincere denials *of what* they believe
indicate that they are unaware *of what* they believe.<15>  It is this
fact -- the fact that these subjects are unaware *of what* they in fact
believe -- that, on the SO account, makes their beliefs unconscious. In
contrast, subjects are conscious *of what* they *consciously* believe,
as evidenced by the fact that, under normal conditions, they are able to
say sincerely and spontaneously what they in fact believe. It is this
fact -- the fact that a subject is conscious *of what* he believes --
that, on the SO account, makes his belief conscious.

So, it appears that the SO account can offer a plausible explanation of
the difference between conscious and unconscious perceptions and
thoughts. But, there are two additional items that need to be mentioned
before the account can be judged fairly.<16>   First, on the SO account,
the state of being conscious *of what* one's token mental state *M*
represents is *caused* by one's mental state *M*. This causal condition
of the SO account is independently plausible and gives a satisfactory
explanation for why one is conscious *of what* *M* represents and not
conscious *of what* some other token mental state (which one might be in
at the time) represents. So, according to this causal condition, your
being conscious *of what* you are seeing is caused by your state of
seeing, which, in turn, explains why you are conscious *of what* you are
seeing and not, say, conscious *of what* you are hearing, smelling, or
believing.

Second, the state of being conscious *of what* one's mental state *M*
represents possesses causal powers distinct from those possessed by the
mental state *M* itself. In linguistic creatures, for instance, a
subject who is conscious *of what* her mental state *M* represents is
able to give a spontaneous (i.e., a non-observationally and non-
inferentially based) report on the content of her mental state *M*;
whereas, a subject who possesses the mental state *M* but who is not
conscious *of what* her mental state *M* represent is not able to give a
spontaneous report on the content of her mental state *M*. A pill
subject in the Nisbett and Wilson experiment who is not conscious *of
what* she actually believes (with regard to the cause of her
shock-related symptoms), for example, is unable to spontaneously say (in
the interview process) what she actually believes when she is asked
about the cause of her shock-related symptoms; whereas, a person who is
conscious *of what* she actually believes with regard to some subject
matter (say, the time of day) is capable of spontaneously saying what
she actually believes about this subject matter (say, "It is now 10:01
a.m.") when asked about it.

In addition, a creature's state of being conscious *of what* its mental
state *M* represents increases the chances that what *M* represents
(i.e., the intentional content of *M*) will be placed in the creature's
short-term memory. A person who is conscious *of what* she believes with
regard to some subject matter, for instance, is much more likely to
remember what she believed about that subject matter than a person who
(like the pill subject's in Nisbett and Wilson's experiment) is not
conscious *of what* she believes with regard to some subject matter. For
it is very difficult to remember what one believed or perceived at a
certain time if at that time one was not conscious *of what* one was
believing or perceiving.

It should also be noted that a creature's state of being conscious *of
what* its mental state *M* represents improves the creature's chances of
adapting to changes in its environment. A cat who sees movement in the
bushes, for instance, but who is not conscious *of what* she is
perceiving -- perhaps, as a result of being momentarily distracted by a
loud noise -- is less likely to catch the mouse in the bushes than the
cat who sees the movement and is conscious *of what* she is perceiving.
And a blindsight subject who is not conscious *of what* she is
perceiving when she is (unconsciously) perceiving an object placed in a
certain (blind) region of her visual field is less likely to respond
adaptively to the object than a normal sighted person who is conscious
*of what* he is perceiving when he is perceiving such an object placed
in his visual field.<17>

Finally, it is important to note that the truth of the SO account is
consistent with the truth of propositions 1, 3, and 4 above. The SO
account does not deny, as the COLD position does, that conscious mental
states are simply mental states of which one is conscious. Rather, the
SO account offers an explanation of what it is for a creature to be
conscious of its mental states. Furthermore, since the SO account does
not explain state consciousness in terms of higher-order inward
consciousness, there is little reason to suppose that it must affirm, as
the inclusive HOT position does, that animals are conscious that they
have mental states, or to deny, as the exclusive HOT position does, that
animals have conscious mental states. Quite the contrary, in fact. Since
there is no apparent reason to deny, and perhaps some rather suggestive
evidence to affirm<18> , that animals are conscious *of what* they think
and experience, there is no apparent reason to think that the SO account
must deny that these creatures have conscious thoughts and experiences.

The SO account, then, offers a way of thinking about state consciousness
that allows us to hold onto some rather intuitively plausible
propositions. This alone, of course, does not prove that such a way of
thinking about state consciousness is correct.<19> But, it does, I believe,
offer a firm motivation to look more closely at the account to see if it
is a viable contender to the established accounts of state consciousness
endorsed by the HOT positions and the COLD position. As things stand,
these established accounts may have reason to worry in the presence of
their new, upstart sparring partner.<20>


NOTES

<1>. For more on these distinctions, and those that follow, see
Rosenthal (1997), Dretske (1995), and Carruthers (2000).

<2>. It should be noted here that by 'inward consciousness' I do not
mean introspection, in the sense of paying deliberate attention to one's
thoughts and experiences. To be inwardly conscious of one's thoughts or
experiences does not require paying deliberate attention to them any
more than being outwardly conscious of some external object (e.g., a
book on a shelf) requires paying deliberate attention to it. See
Rosenthal (1986) for a fuller treatment of the distinction between
introspection and unreflective awareness of one's own mental states,
which I call 'inward consciousness.'

<3>. State consciousness should not to be confused with another
important class of properties of some mental states: phenomenal
properties. Some conscious mental states, such as conscious sensations
or feelings, have phenomenal properties -- that is, there is something
in particular that it is like to have them -- while other conscious
mental states, such as conscious thoughts, do not. There is nothing in
particular that it is like to have a conscious thought; although, there
certainly is something in particular that it is like to have the
conscious feelings and emotions that accompany one's conscious thoughts.

<4>. It should be noted that Carruthers, unlike Rosenthal, endorses a
*dispositional* HOT theory which identifies our consciousness of our
mental states with our being disposed to form immediate higher-order
thoughts about them. Since nothing I say in this paper turns on the
difference between dispositional and non-dispositional HOT accounts, I
take Rosenthal's account as the standard HOT account for the sake of
expository simplicity.

<5>. There is much debate over whether even chimpanzees have thoughts
about phenomena that they cannot observer, such as causal relations and
mental states in conspecifics. So far the evidence is either
controversial or strongly indicative that they do not have such thoughts
(Povinelli, 2000).

<6>. Dretske assumes here that being conscious of one's experience does
not affect (or need not affect) a change *in* one's experience, just as
being conscious of one's cancer does not affect (or need not affect) a
change *in* one's cancer. I shall assume, for the sake of argument, that
he is right on this, provided that 'change in' is short for 'change in
the experience's (or cancer's) *intrinsic* properties.' For being
conscious of one's experience (or cancer) does affect a change in the
experience's (cancer's) *extrinsic* (or relational) properties: they now
have the property of being the object of one's awareness.

<7>. Nor do we see the painting as a painting.

<8>. And, we shall assume, she is not committing a Freudian slip of
tongue.

<9>. Could someone, for instance, sincerely say, "It's raining, but I'm
not aware *of what* I believe with regard to the rain and the present
time"? It seems not. The oddity of assertions of the form, "a is F, but
I am not aware *of what* I believe with regard to a and F," suggests, I
believe, a tight connection between sincerely saying what one believes
and being conscious *of what* one believes.

<10>. So far there is no evidence from developmental psychology that
suggests that children younger than three years are capable of thinking
about their own or other people's beliefs (Clements & Perner, 1994). And
since children younger than three typically do not speak about their own
or other people's beliefs (Shatz, Wellman, & Silber, 1983), it is quite
conceivable that they do not yet think about their own or other people's
beliefs.

<11>. One might raise the following concern here. "In being aware *of
what* she believes, the child is aware of the intentional content of her
belief. But, it is not obvious that this counts as any way at all of
being conscious *of* the mental state, whether "as such" or not." But, I
believe that the reason that things may seem unobvious here is the
result of running the following together: (a) being aware *of what* a
particular representation *r* represents, and (b) being aware of
something which happens to be what a particular representation *r*
represents. To see that these are different, consider the fact that what
one sees on some occasion -- say, an outdoor window scene -- may very
well be what a particular painting *x* represents, and yet it is far
from obvious that this alone means that one sees what painting *x*
represents, or even that one *sees* painting *x*. For one may not even
be looking at painting *x* at the time. Similarly, a child may be aware
of the fact that that is her dolly, and this fact may be what is
represented by her token belief *b*, and yet it is far from obvious that
this alone means that the child is aware of her token belief *b*. I
grant this. But, in the case I describe above, I am assuming that the
child is not simply aware of some fact which happens to be represented
by one of her token beliefs. I am assuming, rather, that the child is
aware *of what* her token belief *b* (the belief that that is her dolly)
represents. And this, I contend, does mean that the child is aware of
her token belief *b* -- though, of course, not as such.

<12>. This is not to say that she must know what it (the distal object)
is that she is seeing -- though, she very well might. Knowing what the
distal object of one's experience is is not the same as being conscious
*of what* (intentional content) one experiences. I might, for instance,
not know what it (the distal object) is that I am perceiving on the side
of the road. Is it a dead animal? A tire? Or am I just suffering from an
illusion? Nevertheless, I am conscious *of what* (intentional content) I
am experiencing: it involves a certain arrangement of various colors,
luminosities, and shapes.

<13>. This is not to say that one is not also conscious of one's
thoughts and experiences when one is conscious *of what* they represent.

<14>. This is not to deny, however, that blindsight subjects are not
also unaware *that* they perceive certain objects or features in their
scotomas. For they certainly are. The point here is simply that it is
not this fact about blindsight subjects that makes their perceptual
states unconscious.

<15>. Again, this is not to deny that such subjects are not also unaware
*that* they hold such beliefs. The point is simply that it is not this
fact about these subjects that makes their beliefs unconscious.

<16>. A more extensive account of these additional items appears in Lurz
(2003).

<17>. Marcel (1998), for instance, remarks that "no matter how well a
perceptual property is non-consciously represented, people who are not
conscious of that perceptual property do not use it in intentional
action and even resist such use" (p. 1585).

<18>. Many animals seem to attend to certain features of what they are
perceiving (Riley & Roitblat, 1978; Andrew, 1976), which suggests that
they are, to some degree, conscious *of what* they are perceiving in
perceiving those features. In addition, there are findings from field
studies on vervet monkeys (Cheney & Seyfarth, 1990) and laboratory
studies on language-trained chimpanzees (Savage-Rumbaugh, 1986) and
parrots (Pepperberg, 1999) that suggest that these animals are capable
of expressing in their proto-languages their beliefs (and not just their
emotional states) about items in their environments. If so, then it
seems plausible to suppose that these animals are, to some degree,
conscious *of what* they believe with respect to these items in their
environments.

<19>. For a fuller defense of the SO account, see Lurz (2003).

<20>. I wish to thank David Rosenthal, Gene Witmer, Murat Aydede, Abe
Witonsky, and M. J. Clarke for their helpful comments on an earlier
draft of this paper.


REFERENCES

Andrew, R. J. (1976). Attentional processes and animal behavior. In P.
     P. G. Bateson & R. A. Hinde (Eds.), *Growing points in ethology*
     (pp. 95-133). Cambridge: Cambridge University Press.

Carruthers, P. (2000). *Phenomenal consciousness*. Cambridge: Cambridge
     University Press.

Cheney, D. & Seyfarth, R. (1990). *How monkeys see the world*. Chicago:
     University of Chicago Press.

Clements, W., & Perner, J. (1994). Implicit understanding of belief.
     *Cognitive Development, 9,* 377-395.

Dretske, F. (1995). *Naturalizing the mind*. Cambridge, MA: MIT
     Press.

Dretske, F. (1997). What good is consciousness? *Canadian Journal of
     Philosophy, 27,* 1-15.

Dretske, F. (1999). The mind's awareness of itself. *Philosophical
     Studies, 95,* 1-22.

Lurz, R. (2003). Advancing the debate between HOT and FO theories of
     consciousness. *Journal of Philosophical Research, 27,* 25-46.

Lycan, W. (1996). *Consciousness and experience*. Cambridge, MA: MIT
     Press.

Marcel, T. (1998). Blindsight and shape perception: Deficits of visual
     consciousness or of visual function? *Brain, 121,* 1565-1588.

Nisbett, R. & Wilson, T. (1977). Telling more than we can know.
     *Psychological Review, 84,* 231-259.

Pepperberg, I. M. (1999). *The Alex studies: Cognitive and communicative
     abilities of grey parrots.* Cambridge, MA: Harvard University
     Press.

Povinelli, D. (2000). *Folk physics for apes*. Oxford: Oxford University
     Press.

Rosenthal, D. (1986). Two concepts of consciousness. *Philosophical
     Studies, 49,* 329-359.

Rosenthal, D. (1997). A theory of consciousness. In N. Block, O.
     Flanagan, & G. Guzeldere (Eds.), *The nature of consciousness* (pp.
     729-753). Cambridge, MA: MIT Press.

Riley, A. R. & Roitblat, H. L. (1978). Selectional attention and related
     cognitive processes in pigeons. In S. H. Hulse, H. Fowler, & W. K.
     Honig (Eds.), *Cognitive processes in animal behavior* (pp.
     249-276). New Jersey: Lawrence Erlbaum Associates.

Savage-Rumbaugh, E. S. (1986). *Ape Language: From conditioned response
     to symbol*. New York: Columbia University Press.

Shatz, M., Wellman, H. M., & Silber, S. (1983). The acquisition of
     mental verbs: A systematic investigation of first references to
     mental states. *Cognition, 14,* 301-321.

Weiskrantz, L. (1986). *Blindsight*. Oxford: Oxford University Press.
